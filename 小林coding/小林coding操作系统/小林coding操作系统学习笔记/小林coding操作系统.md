# 操作系统

---

## 1. 小林coding：如何避免死锁 ?

### 1.1 死锁的概念

多个进程/线程都在等待对方释放锁，在没有外力介入的情况下，这些进程/线程会一直相互等待，没办法执行下去，这种情况就是死锁。

### 1.2 死锁发生的条件

#### 1. 互斥条件

- 多个进程/线程不能同时使用同一个资源。

#### 2.  持有并等待条件

- 某个进程/线程阻塞等待某个资源的同时，还持有另外一个资源。

#### 3. 不可剥夺条件

- 线程持有的资源在自己用完之前是不能被其他线程获取的。

#### 4. 环路等待条件

- 多个线程获取资源的顺序构成了环形链。

![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%AD%BB%E9%94%81/%E7%8E%AF%E8%B7%AF%E7%AD%89%E5%BE%85%E6%9D%A1%E4%BB%B6.png)

### 1.3 模拟死锁问题的产生

```cpp
- 创建 2 个线程，分别为线程 A 和 线程 B
- 然后有两个互斥锁，分别是 mutex_A 和 mutex_B

pthread_mutex_t mutex_A = PTHREAD_MUTEX_INITIALIZER;
pthread_mutex_t mutex_B = PTHREAD_MUTEX_INITIALIZER;

int main()
{
    pthread_t tidA, tidB;

    //创建两个线程
    pthread_create(&tidA, NULL, threadA_proc, NULL);
    pthread_create(&tidB, NULL, threadB_proc, NULL);

    pthread_join(tidA, NULL);
    pthread_join(tidB, NULL);

    printf("exit\n");

    return 0;
}
```

```cpp
//线程 A 函数
void *threadA_proc(void *data)
{
    printf("thread A waiting get ResourceA \n");
    pthread_mutex_lock(&mutex_A);
    printf("thread A got ResourceA \n");

    sleep(1);

    printf("thread A waiting get ResourceB \n");
    pthread_mutex_lock(&mutex_B);
    printf("thread A got ResourceB \n");

    pthread_mutex_unlock(&mutex_B);
    pthread_mutex_unlock(&mutex_A);
    return (void *)0;
}

- 线程 A 函数的过程：
  1. 先获取互斥锁 A，然后睡眠 1 秒；
  2. 再获取互斥锁 B，然后释放互斥锁 B；
  3. 最后释放互斥锁 A；
```

```cpp
//线程B函数
void *threadB_proc(void *data)
{
    printf("thread B waiting get ResourceB \n");
    pthread_mutex_lock(&mutex_B);
    printf("thread B got ResourceB \n");

    sleep(1);

    printf("thread B waiting  get ResourceA \n");
    pthread_mutex_lock(&mutex_A);
    printf("thread B got ResourceA \n");

    pthread_mutex_unlock(&mutex_A);
    pthread_mutex_unlock(&mutex_B);
    return (void *)0;
}

- 线程 B 函数的过程：
  1. 先获取互斥锁 B，然后睡眠 1 秒；
  2. 再获取互斥锁 A，然后释放互斥锁 A；
  3. 最后释放互斥锁 B；
```

运行这个程序，运行结果如下:

thread B waiting get ResourceB 
thread B got ResourceB 
thread A waiting get ResourceA 
thread A got ResourceA 
thread B waiting get ResourceA 
thread A waiting get ResourceB 
// 阻塞中... 

线程 B 在等待互斥锁 A 的释放，线程 A 在等待互斥锁 B 的释放，双方都在等待对方资源
的释放，很明显，产生了死锁问题。

### 1.4 避免死锁问题的产生

办法：破坏产生死锁的条件中的任何一个条件即可，最常见的并且可行的就是**使用资源有序分配法，来<font color="red">破坏环路等待条件</font>。**

#### 什么是资源有序分配法？

线程 A 和 线程 B 总是以相同的顺序申请自己想要的资源。

![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%AD%BB%E9%94%81/%E8%B5%84%E6%BA%90%E6%9C%89%E5%BA%8F%E5%88%86%E9%85%8D.png)

#### 修改1.3代码消除死锁

```cpp
- 修改后：
//线程 B 函数访问资源的顺序，同线程 A 一样，先获取互斥锁 A，然后获取互斥锁 B
void *threadB_proc(void *data)
{
    printf("thread B waiting get ResourceA \n");
    pthread_mutex_lock(&mutex_A);
    printf("thread B got ResourceA \n");

    sleep(1);

    printf("thread B waiting  get ResourceB \n");
    pthread_mutex_lock(&mutex_B);
    printf("thread B got ResourceB \n");

    pthread_mutex_unlock(&mutex_B);
    pthread_mutex_unlock(&mutex_A);
    return (void *)0;
}

- 线程 B 函数的过程：
  1. 先获取互斥锁 A，然后睡眠 1 秒；
  2. 再获取互斥锁 B，然后释放互斥锁 B；
  3. 最后释放互斥锁 A；
```

```cpp
修改前：
//线程B函数
void *threadB_proc(void *data)
{
 printf("thread B waiting get ResourceB \n");
 pthread_mutex_lock(&mutex_B);
 printf("thread B got ResourceB \n");

sleep(1);

printf("thread B waiting get ResourceA \n");
 pthread_mutex_lock(&mutex_A);
 printf("thread B got ResourceA \n");

pthread_mutex_unlock(&mutex_A);
 pthread_mutex_unlock(&mutex_B);
 return (void *)0;
}

- 线程 B 函数的过程：

  1. 先获取互斥锁 B，然后睡眠 1 秒；
  2. 再获取互斥锁 A，然后释放互斥锁 A；
  3. 最后释放互斥锁 B；
```

--- 

## 2. 小林coding：什么是悲观锁、乐观锁？

### 2.1 互斥锁与自旋锁

#### 基础概念

- 互斥锁和自旋锁是**最底层的锁**，是其他高级锁的基础。

- 互斥锁和自旋锁的目的是**保证共享资源**在任何时候都**只能被一个进/线程访问**，避免共享数据因为多进/线程的竞争而错乱。

#### 加锁失败的处理方式

- 当已经有一个线程获取锁后，其他线程获取锁会失败，互斥锁和自旋锁对于获取锁失败后的处理方式是不一样的。

##### 互斥锁的处理方式

- 获取锁失败的线程会释放 CPU ，给其他线程使用。(线程切换)

<img title="" src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%94%81/%E4%BA%92%E6%96%A5%E9%94%81%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png" alt="" width="414" data-align="left">

- 线程获取锁/释放锁时，会发起系统调用，cpu的运行模式会从用户态切换到内核态，让相关内核程序上cpu运行来完成上锁或解锁操作，这样虽然简化了使用锁的难度，但也带来了性能开销成本：**两次线程上下文切换的成本**：
  
  1. 当线程获取锁失败时，内核会把该线程的状态从「运行」状态设置为「睡眠」状态，然后切换其他线程上cpu运行。
  
  2. 当锁被释放时，之前「睡眠」状态的线程会变为「就绪」状态，内核会在合适的时间，切换该线程上cpu运行。

- 线程的上下文切换的是什么？切换的是线程的私有数据、寄存器等不共享数据。

- 上下切换的耗时大概在几十纳秒到几微秒之间，如果<u>锁住的代码</u>（访问共享资源的那段代码）执行时间比较短，那可能上下文切换的时间都比你锁住的代码执行时间还要长。所以，**如果你能确定被锁住的代码执行时间很短，就不应该用互斥锁，而应该选用自旋锁。**

##### 自旋锁的处理方式

- 线程会忙等待，直到它拿到锁。另外，忙等待可以用 `while` 循环实现，不过最好是使用 CPU 提供的 `PAUSE` 指令来实现忙等待，可以减少循环等待时的耗电量。

- 自旋锁是通过 CPU 提供的 `CAS` 函数（*Compare And Swap*），在用户态完成加锁和解锁操作，不会主动产生线程上下文切换，所以相比互斥锁来说，会快一些，开销也小一些。

- 一般加锁的过程，包含两个步骤：
  
  第一步，查看锁的状态，如果锁是空闲的，则执行第二步。
  
  第二步，将锁设置为当前线程持有。
  
  CAS 函数把这两个步骤合并成一条硬件级指令，形成**原子指令**，这样就保证了这两个步骤是不可分割的，要么一次性执行完两个步骤，要么两个步骤都不执行。比如，设**锁为变量 lock**，整数 **0 表示锁是空闲状态**，整数 **pid 表示线程 ID**，那么 **CAS(lock, 0, pid) 就表示自旋锁的加锁操作，CAS(lock, pid, 0) 则表示解锁操作。**
  
  解释说明：
  
  - CAS 操作的一般形式是：CAS(location, expected, new_value)，其中 location 是要修改的共享变量，expected 是期望的值，new_value 是要设置的新值。CAS 操作只有在 location 的当前值等于 expected 时才会成功，否则它将失败。如果成功，它会将 location的值设置为 new_value。
  
  - CAS(lock, 0, pid)表示尝试将 lock的值从 0（表示未锁定状态）更改为pid（表示线程的标识符或锁定者的标识符），以此来实现锁的加锁操作，CAS(lock, pid, 0)表示尝试将 lock的值从 pid 更改回 0，以此来实现锁的解锁操作。

- **需要注意，在单核 CPU 上，需要抢占式的调度器（即不断通过时钟中断一个线程，运行其他线程）。否则，自旋锁在单 CPU 上无法使用，因为一个自旋的线程永远不会放弃 CPU。**

- 自旋锁开销少，在**多核系统下**一般**不会主动进行线程切换**，适合异步、协程等在用户态切换请求的编程方式，但如果某个线程在临界区执行的时间过长，那么自旋的线程会长时间占用 CPU 资源，所以自旋的时间和线程在临界区执行的时间是成「正比」关系。

### 2.2 读写锁

#### 基础概念

读写锁从字面意思我们也可以知道，它由「读锁」和「写锁」两部分构成，如果只读取共享资源那么就用「读锁」来加锁，如果要修改共享资源则用「写锁」来加锁。所以，**读写锁适用于能明确区分读操作和写操作的场景**。    

#### 工作原理

当「写锁」没有被线程持有时，多个线程能够<u>同时持有读锁</u>（因为「读锁」是用于读取共享资源的场景，所以多个线程同时持有读锁也不会破坏共享资源的数据。），这大大提高了共享资源的访问效率。但是，一旦「写锁」被线程持有后，读线程的获取读锁的操作会被阻塞，而且其他写线程的获取写锁的操作也会被阻塞。

所以，**写锁是独占锁**，任何时刻只能有一个线程持有写锁，类似互斥锁和自旋锁，而**读锁是共享锁**，可以被多个线程同时持有。可以发现，**读写锁在读多写少的场景，能发挥出优势**。

#### 读写锁的分类

根据实现的不同，读写锁可以分为「读优先锁」和「写优先锁」。

##### 读优先锁

- 特点：读优先锁希望读锁能被更多的线程持有，**以便提高读线程的并发性**。

- 工作方式：当读线程 A 先持有了读锁，写线程 B 在获取写锁的时候，会被阻塞，在线程B阻塞过程中，后续来的读线程 C 仍然可以成功获取读锁，最后直到读线程 A 和 C 释放读锁后，写线程 B 才可以成功获取写锁。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%94%81/%E8%AF%BB%E4%BC%98%E5%85%88%E9%94%81%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png" title="" alt="" width="383">

##### 写优先锁

- 特点：写优先锁希望先服务写线程。

- 工作方式：当读线程 A 先持有了读锁，写线程 B 在获取写锁的时候，会被阻塞，在阻塞过程中，后续来的读线程 C 获取读锁时会失败，于是读线程 C 被阻塞在获取读锁的操作，这样只要读线程 A 释放读锁后，写线程 B 就可以成功获取写锁。

<img title="" src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%94%81/%E5%86%99%E4%BC%98%E5%85%88%E9%94%81%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png" alt="" width="397">

##### 写优先锁和读优先锁的比较

- 读优先锁可以保证读线程不会饿死，但如果一直有读线程获取读锁，那么写线程将永远获取不到写锁，这就造成了写线程会被饿死。

- 写优先锁可以保证写线程不会饿死，但是如果一直有写线程获取写锁，读线程也会被饿死。

##### 公平读写锁

既然不管读优先锁还是写优先锁，都会造成对方可能会出现饿死的问题，那么我们就不偏袒任何一方，搞个「公平读写锁」。

###### 一种简单的实现方式

用队列把获取锁的线程放在里面排队，不管是写线程还是读线程都按照先进先出的原则加锁即可，这样读线程可以并发并且不会出现饥饿现象。

互斥锁和自旋锁都是最基本的锁，读写锁可以根据场景来选择这两种锁其中的一个进行实现。

---

### 2.3 乐观锁与悲观锁

#### 悲观锁

##### 基本认识

前面提到的互斥锁、自旋锁、读写锁，都是属于悲观锁。

##### 特点

它认为多线程同时修改共享资源的概率比较高，很容易出现冲突，所以访问共享资源前，先要上锁。

#### 乐观锁

##### 基本认识

介于前面提到的悲观锁的特点，那么如果多线程同时修改共享资源的概率比较低，就可以采用乐观锁。

##### 特点

它假多线程访问共享资源起冲突的概率很低，并在此基础上工作。

##### 工作方式

先修改完共享资源，再验证这段时间内有没有发生冲突，如果没有其他线程在修改资源，那么操作完成，如果发现有其他线程已经修改过这个资源，就放弃本次操作。

由此可见，乐观锁的心态是，不管三七二十一，先改了资源再说。另外，**乐观锁全程并没有加锁，所以它也叫无锁编程**。

##### 场景应用例子：在线文档

在线文档可以同时被多人编辑，如果在线文档这个共享资源使用了悲观锁，那么只要有一个用户正在编辑在线文档，此时其他用户就无法打开相同的文档了（因为访问在线文档代码被上锁了），这用户体验当然不好了，所以实现多人同时编辑，实际上是用了乐观锁，它允许多个用户打开同一个文档进行编辑，编辑完提交之后才验证修改的内容是否有冲突。

- 怎么样才算发生冲突？
  
  比如用户 A 先在浏览器上编辑在线文档，之后用户 B 也在浏览器上打开了相同的文档进行编辑，但是用户 B 比用户 A 提交早，这一过程用户 A 是不知道的，当 A 提交修改完的内容时，那么 A 和 B 之间<u>并行修改</u>（同时修改）的地方就会发生冲突。

- 服务端要怎么验证是否冲突了呢？通常方案如下：
  
  由于发生冲突的概率比较低，所以先让用户编辑文档，浏览器在下载文档时会记录服务端发过来的响应中包含的文档版本号。当用户提交修改时，发给服务端的请求会带上记录下来的文档版本号，服务器收到后将自己这里的文档的版本号与用户提交的文档的版本号进行比较，如果版本号不一致则提交失败，如果版本号一致则修改成功，然后服务端文档的版本号更新为新的版本号。

实际上，我们常见的 SVN 和 Git 也是用了乐观锁的思想，先让用户编辑代码，然后提交的时候，通过版本号来判断是否产生了冲突，发生了冲突的地方，需要我们自己修改后，再重新提交。

##### 乐观锁的使用建议

乐观锁虽然去除了加锁解锁的操作，但是一旦发生冲突，重试的成本非常高，所以**只有在冲突概率非常低，且加锁成本非常高的场景时，才考虑使用乐观锁。**

### 2.4 总结

不管使用的哪种锁，代码的加锁范围应该尽可能的小，这样执行速度会比较快，再来，使用上了合适的锁，就会快上加快了。

---

## 3. 小林coding：一个进程最多可以创建多少个线程？

### 3.1 Linux操作系统中的虚拟地址空间

在Linux操作系统中，虚拟地址空间被分为内核空间和用户空间，不同位数的Linux操作系统，地址空间的范围也不同。

<img src="https://cdn.xiaolincoding.com//mysql/other/20210715092026648.png" title="" alt="" width="447">

- 32 位系统的内核空间占 1G ，位于最高处，剩下的 3G 是用户空间;
- 64 位系统的内核空间和用户空间都是 128T ，分别占整个内存空间的最高和最低处，剩下的中间部分是未定义的。

### 3.2 一个进程最多可以创建多少个线程？

这个问题和两个东西相关：

- 进程的虚拟地址空间上限：因为创建一个线程，操作系统需要为其分配一个栈空间（属于用户空间），如果线程数量越多，所需的栈空间就要越大，那么虚拟内存就会占用的越多。

- 系统参数限制：系统参数会控制整个系统的最大线程个数，这些系统参数分别是
  
  - ***/proc/sys/kernel/threads-max***，表示系统支持的最大线程数，默认值是 `14553`；
  
  - ***/proc/sys/kernel/pid_max***，表示系统全局的 PID 号的数量上限，每一个进程或线程都有 ID，ID 的值超过这个数，进程或线程就会创建失败，默认值是 `32768`；
  
  - ***/proc/sys/vm/max_map_count***，表示限制一个进程可以拥有的VMA(虚拟内存区域)的数量，具体什么意思我也没搞清楚，反正如果它的值很小，也会导致创建线程失败，默认值是 `65530`。

### 3.3 为什么物理内存只有 2G，进程的虚拟内存却可以使用 25T 呢？

因为虚拟内存并不是全部都映射到物理内存的，程序是有**局部性**的特性，也就是某一个时间只会执行部分代码，所以只需要映射这部分程序就好，所以虽然进程虚拟空间很大，但是物理内存（RES）只有使用了 400 多M。

<img src="file:///C:/Users/唐飞龙/AppData/Roaming/marktext/images/2023-10-08-08-40-03-image.png" title="" alt="" width="418">

### 3.4 简单总结

- 32 位系统，用户态的虚拟空间只有 3G，如果创建线程时分配的栈空间是 10M，那么一个进程最多只能创建 300 个左右的线程。

- 64 位系统，用户态的虚拟空间大到有 128T，理论上不会受虚拟内存大小的限制，而会受系统的参数或性能限制。

---

## 4. 小林coding：线程崩溃了，进程也会崩溃吗？

### 4.1 线程崩溃，进程一定会崩溃吗

一般来说，如果是**线程非法访问共享地址空间**引起的线程崩溃，那么**该线程属于的进程一定会崩溃**。因为线程之间共享进程的内存地址空间，而其中的某个线程如果对该地址空间进行了非法访问就会导致内存充满不确定性，进而影响到其他线程的运行，于是操作系统就认为该线程的行为十分危险，可能会产生一系列严重的后果，于是就直接让该线程属于的进程崩溃。

**解释说明：**

- **内存的不确定性**是指内存在被访问或者操作之后，内存中存放的访问/操作之后的结果没办法预测。

- 非法访问是指会引发错误的访问。

**非法访问内存有以下几种情况：**

1.、针对只读内存写入数据：

![](C:\Users\唐飞龙\AppData\Roaming\marktext\images\2023-10-07-17-37-56-image.png)

2、访问了进程没有权限访问的地址空间（比如内核空间）：

![](C:\Users\唐飞龙\AppData\Roaming\marktext\images\2023-10-07-17-38-18-image.png)

在 32 位虚拟地址空间中，p 指向的是内核空间，显然不具有写入权限，所以上述赋值操作会导致进程崩溃

3、 访问了不存在的内存：

![](C:\Users\唐飞龙\AppData\Roaming\marktext\images\2023-10-07-17-38-51-image.png)

### 4.2 Linux操作系统中进程崩溃的原理

#### 信号机制介绍

在Linux系统中，终止运行中的进程会使用到kill -9 pid这样的命令。先介绍一下kill -9 pid各个部分是什么意思：

- ``kill``是一个用于向进程发送信号的命令，在Linux系统中的主要作用是**与正在运行的进程进行交互，以请求<u>它们</u>（上文说的进程）执行不同的操作**，包括终止、重启、暂停、继续等。

- ``-9``是kill命令的选项，表示要发送的信号是SIGKILL，也就是指出kill命令要发送的信号是什么。SIGKILL是一个特殊的信号，它会立即终止目标进程，不会给进程执行清理或处理的机会，这是一种强制终止进程的方式。

- pid是要终止的进程的PID。

介绍完毕，继续阐述。做出发送信号这个行为是需要具有权限的，否则任意进程都可以发送信号来终止其他进程，这显然是不合理的，所以kill 命令本身其实是发出系统调用，即使用kill命令来请求操作系统内核发送信号给进程。

#### 信号能够引起进程崩溃的原理

1. CPU 执行正常的进程指令
2. 发出kill 系统调用
3. 当前进程被中断，保存运行环境，CPU的使用权转交给操作系统
4. 操作系统执行kill 系统调用向进程发送信号（假设为 11，即 SIGSEGV信号，一般非法访问内存都是发送这个信号），信号的信息（例如信号编号）通常会添加到进程的进程控制块中，以便进程在稍后的某个时间点可以检查这个信号。
5. 操作系统在确定进程已经接收到信号后，**根据情况执行相应的信号处理程序（函数），一般信号处理程序执行完逻辑后进程就会退出**，如果进程没有注册自己的信号处理函数，那么操作系统会执行默认的信号处理程序（一般最后会让进程退出），但如果注册了，则操作系统会调用进程的信号处理函数，**但也可以使用 sigsetjmp，siglongjmp 这两个函数来恢复进程的执行**。

![](C:\Users\唐飞龙\AppData\Roaming\marktext\images\2023-10-07-17-19-40-image.png)

另外当进程接收信号之后也可以不定义自己的信号处理函数，而是选择忽略信号，如下：

![](C:\Users\唐飞龙\AppData\Roaming\marktext\images\2023-10-07-17-20-56-image.png)

也就是说虽然给进程发送了 kill 信号，但如果进程自己定义了信号处理函数或者无视信号就有机会逃出生天，当然了 kill -9 命令例外，不管进程是否定义了信号处理函数，都会马上被干掉。

#### 如何让正在运行的程序优雅结束？（解除资源占用）

根据上面的解释可以知道，运行的进程自己定义了信号处理函数，这样当发送 kill pid 命令（默认会传 15 也就是 SIGTERM）后，运行的进程 就可以**在信号处理函数中执行一些资源清理之后再调用 exit 退出**。这种场景显然不能用 kill -9，不然一下把进程干掉了资源就来不及清除了。

### 4.3 为什么栈溢出（Stackoverflow）属于非法访问内存呢？

#### 进程的虚拟空间（也就是前面提到的共享地址空间）的介绍

- 现代操作系统为了保护进程之间不受影响，使用了虚拟地址空间来隔离进程，进程的寻址都是针对虚拟地址，每个进程的虚拟空间都是一样的，而线程会共用进程的地址空间。

- 在32位操作系统下的进程的虚拟空间分布图：

![](https://cdn.xiaolincoding.com//mysql/other/8de250fcb055400c94f95c99712a1158.png)

#### stackoverflow 是怎么发生的呢？

首先我们需要知道，进程每调用一个函数，都会分配一个栈桢，然后在栈桢里会分配函数里定义的各种局部变量。

假设现在调用了一个无限递归的函数，那就会持续分配栈帧，但 stack 的大小是有限的（Linux 中默认为 8 M，可以通过 ulimit -a 查看），如果无限递归，栈很快就会被分配完，此时再调用函数试图分配超出栈的大小的内存地址空间，就会发生段错误，也就是 stackoverflow Error。

![](https://cdn.xiaolincoding.com//mysql/other/c54aff1660e34d8a8a83d534c3390954.png)

为什么发生栈溢出的线程的崩溃不会导致JVM退出呢？

**基础概念：**

- 在启动运行环境的时候，JVM会设置自定义信号处理函数，在收到 SIGSEGV，SIGPIPE 等信号后最终会调用自定义信号处理函数。

- 在线程引发 stackoverflow 错误时，操作系统会向运行时环境发送 SIGSEGV并调用JVM的自定义信号处理函数，在该处理函数中，JVM不选择退出，而是进行额外的处理，即恢复产生栈溢出的线程执行（从断点开始执行），并抛出Stackoverflow Error，这就是为什么运行环境不会崩溃且我们能捕获到该错误的原因。

- 如果针对 SIGSEGV 等信号，JVM没有做额外的处理，那么最终会执行生成 hs_err_pid_xxx.log crash 文件（记录了一些堆栈信息或错误）的方法，然后退出。

**总结：原因其实就是虚拟机内部定义了信号处理函数，而在信号处理函数中对这两者做了额外的处理以让 JVM 不崩溃，另一方面也可以看出如果 JVM 不对信号做额外的处理，最后会自己退出并产生 crash 文件 hs_err_pid_xxx.log（可以通过 -XX:ErrorFile=/var/*log*/hs_err.log 这样的方式指定），这个文件记录了虚拟机崩溃的重要原因。**

---

## 5. 小林coding：为什么要有虚拟内存？

### 虚拟内存的由来

当多个进程运行在内存中时，如果这些进程同时访问到同一个物理内存地址，那么必然会引起地址冲突，从而导致错误，操作系统为了避免这一个问题，于是为每个进程分配专属于自己的虚拟内存空间，使得它们不能直接访问物理内存地址而是访问各自的虚拟内存地址，互不干扰，然后提供一种机制，将不同的虚拟内存地址映射到不同的物理内存地址上，这样多个进程同时运行在内存中就不会出现地址冲突的问题了。

### 虚拟内存地址和物理内存地址

#### 概念

- 虚拟内存地址：程序使用的地址

- 物理内存地址：硬件内部空间的地址

#### 虚拟内存地址映射到物理内存地址的原理

进程的虚拟地址会通过 CPU 芯片中的内存管理单元（MMU）中的映射关系，来转换成物理地址，然后再通过物理地址访问内存。

<img src="https://cdn.xiaolincoding.com//mysql/other/72ab76ba697e470b8ceb14d5fc5688d9.png" title="" alt="" width="312">

内存管理单元中存放有页表，它的其中一个作用就是地址转换，另外一个作用是页面置换。

### 管理物理内存的方式

#### 内存分段

##### 概念

程序由若干逻辑段组成，如代码段，数据段，栈段，堆段等。因为不同的逻辑段的特性不同，所以可以采用分段的形式把这些段分离出来。

##### 分段机制下，虚拟地址到物理地址的映射

虚拟地址被划分为两个部分：**段选择因子**和**段内偏移量** 

- 段选择因子保存在段寄存器中。段选择因子中最重要的是段号，作为索引来查找段表项。段表里面的段表项包含段的基地址、段的界限和特权等级等。

- 段内偏移量的值在[ 0 ,段的界限]，如果段内偏移量合法，段基地址加上段内偏移量就能得到物理内存地址。

<img title="" src="https://cdn.xiaolincoding.com//mysql/other/a9ed979e2ed8414f9828767592aadc21.png" alt="" width="386">

虚拟地址通过**段表**与物理地址进行映射，分段机制把程序的虚拟地址分成 4 个段，每个段在段表中有一个段表项，在这一项找到段的基地址，再加上段内偏移量，就能找到物理内存地址。

<img title="" src="https://cdn.xiaolincoding.com//mysql/other/c5e2ab63e6ee4c8db575f3c7c9c85962.png" alt="" width="326">

##### 不足

- 产生外部内存碎片以及内存交换的效率低。
- 内存碎片主要分为，内部内存碎片和外部内存碎片。内部内存碎片是指分配的内存块内部空间大于实际需求，存在没有被利用到的内存空间；外部内存碎片是指空闲内存无法装入外部程序。

**分段为什么会产生外部内存碎片？** 每个段的长度不固定，所以多个段未必能恰好使用完所有的内存空间，会产生了多个不连续的小的物理内存，导致新的程序无法被装载。

    <img title="" src="https://cdn.xiaolincoding.com//mysql/other/6142bc3c917e4a6298bdb62936e0d332.png" alt="" width="422">

**如何解决外部内存碎片的问题？通过内存交换。**

我们可以<u>把音乐程序占用的那 256MB 内存写到硬盘上，然后再从硬盘上读回来到内存里</u>（内存交换）。不过再读回的时候，我们不能装载回原来的位置，而是紧紧跟着那已经被占用了的 512MB 内存后面。这样就能空缺出连续的 256MB 空间，于是新的 200MB 程序就可以装载进来。

在 Linux 系统里，这个用于进行内存交换的空间，就是我们常看到的 Swap 空间，而这块空间是从硬盘划分出来的，用于内存与硬盘的空间交换。

**分段为什么会导致内存交换效率低？**

对于多进程的系统来说，采用分段的方式很容易产生外部内存碎片，产生了外部内存碎片，那就不得不进行内存交换操作，但因为<u>硬盘的访问速度要比内存慢太多了，每一次内存交换，我们都需要把一大段连续的内存数据写到硬盘上。</u>所以，**如果内存交换的时候，交换的是一个占内存空间很大的程序，这样整个机器都会显得卡顿。**

#### 内存分页

##### 概念

- 相比较于内存分段，内存分页的好处就是 ：1. 降低了外部内存碎片的出现  2. 在进行内存交换的时候，让需要写入或者从磁盘装载的数据更少了。

- **分页是把整个虚拟和物理内存空间分成一段段固定尺寸的大小**。这样一个连续并且尺寸固定的内存空间被称为**页**（*Page*）。在 Linux 下，每一页的大小为 `4KB`。

##### 分页机制下，虚拟地址到物理地址的映射

虚拟地址与物理地址之间通过**页表**来映射，页表存储在内存里，**内存管理单元** （*MMU*）将虚拟内存地址转换成物理地址。

<img src="https://cdn.xiaolincoding.com//mysql/other/08a8e315fedc4a858060db5cb4a654af.png" title="" alt="" width="388">

虚拟地址分为两部分，**页号**和**页内偏移**。页号作为页表的索引，**页表**包含物理页每页所在**物理内存的基地址**，这个基地址与页内偏移的组合就形成了物理内存地址，见下图。

<img src="https://cdn.xiaolincoding.com//mysql/other/7884f4d8db4949f7a5bb4bbd0f452609.png" title="" alt="" width="395">

对于一个内存地址转换，只需三个步骤：

- 把虚拟内存地址，切分成页号和偏移量；
- 根据页号，从页表里面，查询对应的物理页号；
- 直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。

**简单分页**

<img src="https://cdn.xiaolincoding.com//mysql/other/8f187878c809414ca2486b0b71e8880e.png" title="" alt="" width="303">

简单分页放到实际中操作系统会有问题，存在空间上的缺陷。因为操作系统是可以同时运行非常多的进程，这就意味着光是这些进程的页表会占用很多内存空间。

**多级页表**

解决上面的问题，就需要采用一种叫作**多级页表**的解决方案。

**实现方法**

将一级页表分为 `1024` 个页表（二级页表），每个二级页表包含 `1024` 个「页表项」，形成**二级分页**。如下图所示：

<img src="https://cdn.xiaolincoding.com//mysql/other/19296e249b2240c29f9c52be70f611d5.png" title="" alt="" width="308">

当进程访问的虚拟地址在页表中查不到时，系统会产生一个**缺页异常**，然后进入内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行。

**分页是怎么解决分段的「外部内存碎片和内存交换效率低」的问题？**

- 因为页与页之间是紧密排列的，不会像内存分段一样，段与段之间会产生间隙非常小的内存，所以分页不会有外部碎片。但是，因为内存分页机制分配内存的最小单位是一页，即使程序不足一页大小，按一页进行内存分配，所以页内会出现内存浪费，所以针对**内存分页机制会有内部内存碎片**的现象。

- 内存空间不够，操作系统会把其他正在运行的进程中的「最近没被使用」的内存页面暂时写到硬盘，这称为**换出**（*Swap Out*）。一旦需要的时候，再加载进来，这称为**换入**（*Swap In*）。所以，一次性写入磁盘的也只有少数的一个页或者几个页，不会花太多时间，**因此内存交换的效率就相对比较高。**

        <img src="https://cdn.xiaolincoding.com//mysql/other/388a29f45fe947e5a49240e4eff13538-20230309234651917.png" title="" alt="" width="372">

分页的方式使得我们在加载程序的时候，不需要把程序的所有页一次性加载到物理内存中来，可以先加载运行时必须的页到内存中去，**在程序运行的过程中，再将需要用到的页加载到物理内存里面去。** 未被加载到物理内存中的页存放在进行内存交换的空间中，而页面划分和创建页表是在创建进程的时候进行。
